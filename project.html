How it works
Data fusion: The core of multimodal segmentation is fusing information from different sources. This can be done at different stages of the process:
Input-level fusion: Combining raw images from different modalities before they enter the segmentation network.
Layer-level fusion: Merging features extracted at different layers within the neural network.
Decision-level fusion: Combining the outputs or decisions from separate segmentation models, each trained on a different modality.
Neural networks: Deep learning models, particularly convolutional neural networks (CNNs) and transformers, are used to process and combine these different data streams.
Feature extraction: The networks are designed to extract relevant features from each modality and then combine them to create a more robust and comprehensive representation of the image content.
Modality-specific challenges: Researchers are developing methods to handle challenges like soft tissue deformation, which can cause images from different modalities to be slightly misaligned, by using techniques that learn shared representations. 